---
# title: Interactions and factors in lm
# latest-revision: November 30th, 2023
# author: Vittorio Zampinetti
# time: 1.5h
# output: pdf_document
---

# Interactions and qualitative predictors

## Transformations

Formulae in R linear models are much more powerful than
what seen so far. In some cases we might be interested in
transforming a variable before fitting the linear model.

### A simple example

For instance, let's take this synthetic dataset:

```{r}
library(tibble)
n <- 100
synth <- tibble(
  x = seq(from = 1, to = 30, length.out = n),
  y = log(x) + rnorm(n, 0, 0.2)
)
```

which looks like this:

```{r}
library(ggplot2)
ggplot(synth) +
  geom_point(aes(x, y))
```

Of course, we can try to fit a linear model without any
extra effort

```{r}
naive_lm <- lm(y ~ x, data = synth)
ggplot(naive_lm, mapping = aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red")
```

but if we plot the residuals, we can detect some issues for low
values of $y$ (definitely not uncorrelated).

```{r}
library(dplyr)
synth %>%
  mutate(res = naive_lm$residuals) %>% # adds the residual column
  ggplot() +
  geom_point(aes(y, res))
```

By understanding how $x$ is distributed, we can fix this issue
and fit the model on a transformation of itself, clearly $\log(x)$.
We do this simply by adding the desired transformation in the 
formula, meaning that we don't have to tranform the dataset beforehand.

```{r}
log_lm <- lm(y ~ log(x), data = synth)
ggplot(log_lm,
  mapping = aes(`log(x)`, y)
) + # notice the backticks!
  geom_point() +
  geom_smooth(method = "lm", color = "red")
```

and the residuals plot.

```{r}
synth %>%
  mutate(res = log_lm$residuals) %>% # adds the residual column
  ggplot() +
  geom_point(aes(y, res))
```

### On _advertising_

Back to our real dataset.

```{r}
library(readr)
advertising <- read_csv("./datasets/advertising.csv")
```

Let's try applying a transformation to the most promising
predictor, in particular let's us $\sqrt{TV}$. The choice
of the square root transformation comes from a first look
at the scatter plot shown previously (TV against Sales),
where we can detect a slightly curved trend which resembles
a curve $y = \sqrt{x}$.

Let's plot the data after the transformation and observe
that its trend better fit a straight line.

```{r}
advertising %>%
  dplyr::select(Sales, TV) %>%
  dplyr::mutate(sqrtTV = sqrt(TV)) %>%
  ggplot(aes(sqrtTV, Sales)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red")
```

We can fit a linear model. Its `summary` table
will show a better $R^2$ score.

```{r}
trans_lm <- lm(Sales ~ sqrt(TV), data = advertising)
```

> *Exercise*: plot the regression line and compare it with
the simple model fitted on the raw data

The following two commands might help in inspecting
a linear model with transformed data.

```{r}
head(model.matrix(trans_lm)) # prints the design matrix
trans_lm$terms # inspect the linear model specifics
```

## Model selection

### R-squared

One of the indicators computed with `summary` on a fitted linear model
is the $R^2$ index (R-squared, or coefficient of determination). It is defined as

$$
R^2 = 1 - \frac{\sum_i (y_i - \hat y_i)^2}{\sum_i (y_i - \bar y)^2}
$$

and shows the impact of the residuals proportionately to the variance
of the response variable. It can be used to compare different models
although it should not be considered as an absolute score of the model.
The closer it is to 1, the better is the model fit.

This output, for instance, shows how the transformation used above 
seems to improve the accuracy of the regression task.

```{r}
simple_lm <- lm(Sales ~ TV, data = advertising)
summary(simple_lm)$r.squared
summary(trans_lm)$r.squared
```

### ANOVA

Another way of comparing two models, in particular one model with
a smaller *nested* model, is the ANOVA test, which is an instance
of the F-test, with statistics

$$
F = \frac{\left(\frac{RSS_1 - RSS_2}{p_2 - p_1}\right)}{\left(\frac{RSS_2}{n - p_2}\right)}
$$

where $RSS$ is the residual sum of squares and $p_2 > p_1$ (model 1 is
smaller). We know from theory that $F \sim F(p_2 - p_1, n - p_2)$ 

A low p-value for the F statistic means that we can reject the hypothesis that
the smaller model explains the data well enough.

Here we compare the model with a single predictor (squared root of TV), which
is the *smaller* model, with a model with also the Radio data as 
additional predictor.

```{r}
double_lm <- lm(Sales ~ sqrt(TV) + Radio, data = advertising)
anova(trans_lm, double_lm)
```

> *Exercise*: compute both the F-statistics and the associated p-value
"*by hand*" and verify your solution with the anova summary table.

## Qualitative predictors

We recycle a textbook example taken from
McClave JT., Benson PG. e Sincich T. (2014). 
*Statistics for Business and Economics.* 
Pearson Education Limited.   
We want to study the effect on the response (variable *distance*)
of 4 different brands of golf ball (A,B,C,D) (variable *brand*)
and the club  type (DRIVER/IRON) (variable *club*).
These two features are qualitative predictors, also called factors.
A robot player is used.

```{r}
wide_golf <- read_tsv("./datasets/golfer.tsv")
spec(wide_golf) # prints information about the detected data types
```

Although it's not always necessary, it is always best to
explicitly tell R to interpret qualitative predictors data as factors.
This can be done with `read_tsv`, first by reading the data as it is
and then calling the `spec()` function over the new tibble.

In this case, the output is saying that the first column has
been detected as `numeric`, which is false, because the golfer
number is just an identification number.
Therefore we correct this by copying the output, manually editing
the first column type from `col_double()` to `col_factor()` and
setting the `read_tsv` parameter `col_types` to that.

```{r}
wide_golf <- read_tsv("./datasets/golfer.tsv",
  col_types = cols(
    club = readr::col_factor(),
    A = col_double(),
    B = col_double(),
    C = col_double(),
    D = col_double()
  )
)

library(dplyr)
# need to switch from wide to long format
golf <- wide_golf %>%
  gather(brand, distance, A:D)

## if using data.frame, you can use `melt()` from
## reshape2
# golf <- melt(wide_golf, id.vars = 1,
#             variable.name = "brand",
#             value.name = "distance")
```

Let us study first the effect of *brand* on the *distance*.
```{r}
lm_brand <- lm(distance ~ brand, data = golf)
summary(lm_brand)
```
```{r}
model.matrix(lm_brand) # this is the design matrix
```

### Data visualization

To visualize the data, we can use boxplots or more specialized
graphics for qualitative predictors (for the latter, 
we have to define the predictors explicitly as factor 
objects in R). Recall that prettier graphics can always
be produced using *ggplot2*.

```{r}
boxplot(distance ~ brand, data = golf)
golf$brand <- as.factor(golf$brand)
plot.design(distance ~ brand, data = golf)
```

## Interactions

Interactions are added in the `lm` formula. More specifically:


- `a:b` (colon op) includes the cross-variable between two predictors
- `a*b` (asterisk op) includes the two predictors individually and the
  cross-variable (i.e. writing `y ~ a + b + a:b` is equivalent to 
  writing `y ~ a*b`)

Back to our dataset, let us now add the second factor *club* and build a 'small'
additive model...

```{r}
small_lm <- lm(distance ~ brand + club, data = golf)
summary(small_lm)
```
... and a larger one with interaction.

```{r}
large_lm <- lm(distance ~ brand * club, data = golf)
summary(large_lm)
```

An interaction plot suggests interactions may be significant
(the lines are not parallel),
but we have to test for it since interaction plots are
subject to sampling variation.

```{r}
plot.design(distance ~ brand * club, data = golf)
interaction.plot(golf$brand, golf$club, golf$distance)
```

## Model comparison

We can compare the complete model to a smaller one
with ANOVA test.

```{r}
anova(large_lm)
```

With the last *anova()* command we have built a more
traditional anova table.
We can compare the two models also with the *anova* command.

```{r}
anova(small_lm, large_lm)
```

Interactions are significant, meaning each combination of the two factors
tells a different story.

```{r}
fitted(large_lm)
```

## Predict

Especially when dealing with qualitative data, predictions for
new unseen data can be easily computed with the `predict` function,
which simply applies the regression coefficients to the provided data
(arbitrarily generated below inside a tibble).

```{r}
predict.lm(small_lm,
  newdata = data.frame(
    club = c("DRIVER", "IRON", "IRON"),
    brand = c("A", "B", "B")
  ),
  interval = "confidence"
)

# for prediction intervals...
predict.lm(large_lm,
  newdata = data.frame(club = "DRIVER", brand = "A"),
  interval = "prediction",
  level = .99
)
```
