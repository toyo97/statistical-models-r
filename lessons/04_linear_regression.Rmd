---
# title: Linear regression
# latest-revision: November 14th, 2023
# author: Vittorio Zampinetti
# time: 1.5h
# output: pdf_document
---

# The linear model

During this lecture, we will use the *Advertising* dataset,
which can be found [in Kaggle](https://www.kaggle.com/datasets/ashydv/advertising-dataset).
It shows, for each line, the revenue (`Sales`), which is the *response*  depending on 
three *predictors*, i.e. money spent on three different advertising channels: `TV`, `Radio`, `Newspaper`.
Every variable is continuous, allowing to explain the main R
functions used in a plain linear regression task.

## Dataset import
Import the dataset (you may have to select properly the current working directory).
Remember a modern version of a dataset in R is called *tibble* (as opposed to the 
previous *dataframe*)
```{r}
library(readr)
# here the wd contains a folder 'dataset' which
# in turn contains the file to be read
advertising <- read_csv("./datasets/advertising.csv")
advertising
```

## Basic EDA (Exploratory Data Analysis)

Get some basic statistics with `summary()`
```{r}
summary(advertising)
```

You can `attach()` the dataset to access columns writing  less code, although this is not 
recommended nowadays (see `with()` below). All the columns of the table are then
available in the R environment without prepending the dataset name
```{r}
attach(advertising)
head(TV)
```

To revert back, detach the dataset
```{r}
detach(advertising)
```

It is better to use `with()` instead.
This function basically attaches a certain context object
(e.g. the dataset namespace), executes the commands inside
the curly brackets, and then detaches the context object.
```{r}
with(advertising, {
  print(head(Sales))
  print(mean(TV))
  print(Radio[Radio < 20])
})
```

These are some of the functions that might be useful
when gathering information about a dataset.
```{r}
length(advertising) # columns!
nrow(advertising)
dim(advertising)
names(advertising)
```

## Simple plots

When exploring a dataset, `ggplot()` might be a bit excessive
(recall that `ggplot()` is a modern way to draw cool plots using
a graphical syntax). Faster plots can be drawn with `plot()` and `pairs()`
```{r}
plot(advertising)
```

A pair-plot gives a bigger picture of the
cross correlations between variables.
The base R `pairs()` function can be tweaked (check `?pairs`)
in order to draw other kind of information of interest.

```{r}
## put histograms on the diagonal
panel_hist <- function(x, ...) {
  usr <- par("usr")
  par(usr = c(usr[1:2], 0, 1.5))
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks
  nb <- length(breaks)
  y <- h$counts
  y <- y / max(y)
  rect(breaks[-nb], 0, breaks[-1], y, col = "cyan", ...)
}
## put (absolute) correlations on the upper panels,
## with size proportional to the correlations.
panel_cor <- function(x, y, digits = 2, prefix = "", cex_cor, ...) {
  par(usr = c(0, 1, 0, 1))
  r <- abs(cor(x, y))
  txt <- format(c(r, 0.123456789), digits = digits)[1]
  txt <- paste0(prefix, txt)
  if (missing(cex_cor)) cex_cor <- 0.8 / strwidth(txt)
  text(0.5, 0.5, txt, cex = cex_cor * r)
}

pairs(advertising,
  upper.panel = panel_cor, diag.panel = panel_hist
)
```

With much less effort, we can obtain a prettier
version of the pair-plot.
```{r}
library(GGally)
ggpairs(advertising, progress = FALSE)
```

Again, with base R let's plot the predictors against the response.
We know how to do that with a single predictor.
```{r}
plot(advertising$TV, advertising$Sales)
```

In case we want to add all plots to a single figure,
we can do it as follow (base R)
```{r}
# set plot parameters
with(advertising, {
  par(mfrow = c(1, 3))
  plot(TV, Sales)
  plot(Radio, Sales)
  plot(Newspaper, Sales)
})
```

adding a smoothed line would look like this
```{r}
# set plot parameters
with(advertising, {
  par(mfrow = c(1, 3))
  scatter.smooth(TV, Sales, col = "red")
  scatter.smooth(Radio, Sales, col = "red")
  scatter.smooth(Newspaper, Sales, col = "red", span = .3) # tweak with span
})
```

The same pictures can also be plotted with `ggplot()`,
if preferred (in two ways).

```{r}
library(ggplot2)

ggplot(advertising) +
  geom_point(aes(TV, Sales))
```

First method: draw three separate plots and arrange
them together with `ggarrange()`

```{r}
library(ggpubr)
tvplt <- ggplot(advertising, mapping = aes(TV, Sales)) +
  geom_point() +
  geom_smooth()
radplt <- ggplot(advertising, mapping = aes(Radio, Sales)) +
  geom_point() +
  geom_smooth()
nwsplt <- ggplot(advertising, mapping = aes(Newspaper, Sales)) +
  geom_point() +
  geom_smooth()

ggarrange(tvplt, radplt, nwsplt,
  ncol = 3
)
```

Second method: rearrange the dataset first,
and feed everything to `ggplot()`.

```{r}
library(tibble)
library(tidyr)

## if using melt, need to switch to data.frame
# library(reshape2)
# advertising <- as.data.frame(advertising)
# long_adv <- melt(advertising, id.vars = "Sales")

long_adv <- advertising %>%
  gather(channel, value, -Sales)
head(long_adv)
```
```{r}
ggplot(long_adv, aes(value, Sales)) +
  geom_point() +
  geom_smooth() +
  facet_wrap(~channel, scales = "free")
```

If you're not sure why we need to use the `melt()` function,
check the first lab (Iris dataset), or simply read the `?melt`
helper.

## Simple regression

The core of this class is the `lm()` function: its first argument is a *formula*, 
centered around the symbol ~, whith a response on its left and a list of predictors on its right.

Let's start with a simple single quantitative predictor linear
model (simple linear regression)

```{r}
simple_reg <- lm(Sales ~ TV, data = advertising)
```
Now that we fitted the model, we have access to the
coefficient estimates and we can plot the *estimated* regression
line.

### Plot

Here two ways of drawing the regression line,
first in base R
```{r}
with(data = advertising, {
  plot(x = TV, y = Sales)
  # abline draws a line from intercept and slope
  abline(
    simple_reg,
    col = "red"
  )
})
```

and in ggplot

```{r}
ggplot(simple_reg, mapping = aes(TV, Sales)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red")
```

Actually, `lm()` does a lot more than just compute the least squares coefficients.

```{r}
summary(simple_reg)
```

We will analyze in detail this output in the next subsection.


### Summary

`summary(lm)` contains information about the following quantities:

- residuals: $Y - X\hat\beta$
- estimated coefficients: $\hat\beta$
- (estimated) standard errors on the coefficients (in the `Std. Error` column) $S \sqrt{(X'X)^{-1}_{i+1,i+1}}$
  because $\frac{\hat\beta_i - \beta_i}{S\sqrt{(X'X)^{-1}}}\sim t(n-p)$
- in the `t value` column: value of the test statistic for the null hypothesis $H_0: \beta_{i+1} = 0$
- in the `Pr(>|t|)` column: p-value regarding the null hypothesis above

- residual std error: $S = \sqrt{\frac{e'e}{n-p}}$, i.e. the square root of the Mean Square Residual (MSR)
- multiple R squared: *later*
- adj R squared: *later*

Let's compute them to make sure we understood the concepts

#### Residuals

Easy to retrieve them (actually, 
they are the realization of the residuals)

$$
e = y - X\hat\beta
$$

```{r}
y <- advertising$Sales
x <- cbind(1, advertising$TV)
e <- y - x %*% simple_reg$coefficients
head(tibble(
  lm_res = simple_reg$residuals,
  manual_res = as.vector(e)
))
```

#### Estimate

Estimates are just the $\hat\beta$ values for each predictor (plus intercept).
They are computed with the closed form max likelihood formula.

$$
\hat\beta = (X'X)^{-1}X'Y
$$

```{r}
beta_hat <- solve(t(x) %*% x) %*% t(x) %*% y
head(tibble(
  lm_coeff = simple_reg$coefficients,
  manual_coeff = as.vector(beta_hat)
))
```

#### Standard Error

For each predictor, this quantifies the estimated variation in the beta estimator.
The lower the standard error is, the higher is the accuracy of that particular coefficient.
For predictor $i$, it is computed as

$$
SE_i = S \sqrt{(X'X)^{-1}_{i+1,i+1}}
$$
$i+1$ is used here since first column if for the intercept $\beta_0$.

```{r}
n <- nrow(x)
p <- ncol(x)
rms <- t(e) %*% e / (n - p)

# SE for TV
tv_se <- sqrt(rms * solve(t(x) %*% x)[2, 2])
tv_se
```

#### T-value and p-value

These two are related to each other. The first is the
test statistics value, under the null hypothesis
$H_0 : \beta_i = 0$, for the variable

$$
\frac{\hat\beta_i - \beta_i}{S \sqrt{(X'X)^{-1}}}
$$

which is student-T distributed with $n-2$ degrees of freedom.

```{r}
# for TV
t_val <- simple_reg$coefficients[2] / tv_se
t_val
```

And finally, the p-value is the probability
on a $t(n-2)$ distribution of the statistic to be
beyond the value actually observed. Remember that with *beyond*
we mean on both sides of the distribution, since the 
alternative hypothesis $H_1: \beta \neq 0$ is two-sided.

```{r}
# multiply by two because the alternative hyp is two-sided
p_val <- 2 * pt(t_val, n - 2, lower.tail = FALSE)
p_val
```
Here we cannot appreciate the manual computation since the
p-value is very low (meaning that we can reject the null,
favoring the alternative).

Exercise: You can try to compute this value manually on another
simple regression model where we use `Radio` as predictor.

```{r}
radio_simple_reg <- lm(Sales ~ Radio, data = advertising)
summary(radio_simple_reg)
```

