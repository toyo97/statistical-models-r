---
title: Linear regression
latest-revision: November 14th, 2023
author: Vittorio Zampinetti
time: 1.5h
output: pdf_document
---

```{r, echo=FALSE}
library(knitr)
opts_knit$set(root.dir = "../")
```

# The linear model

For this lecture, we will use the *Advertising* dataset,
which can be found [here](https://www.kaggle.com/datasets/ashydv/advertising-dataset).
It shows, for each line, the revenue (`Sales`) depending on the
money spent on three different advertising channels: `TV`, `Radio`, `Newspaper`.

Every attribute is continuous, and allows to explain the main R
functions used in a plain linear regression task.

## Dataset import
Import the dataset (remember to check the current working
directory).
```{r}
library(readr)
# here the wd contains a folder 'dataset' which
# in turn contains the file to be read
advertising <- read_csv("./datasets/advertising.csv")
advertising
```

## Basic EDA

Get some statistics with `summary()`
```{r}
summary(advertising)
```

You can attach the dataset to access columns with less code 
(not recommended). All the columns of the table are then
available in the R environment without prepending the dataset name
```{r}
attach(advertising)
head(TV)
```

To revert back, detach the dataset
```{r}
detach(advertising)
```

It is better to use `with()` instead, if really needed.
This function basically attach a certain context object
(e.g. the dataset namespace), executes the commands inside
the curly brackets, and then detaches the context object.
```{r}
with(advertising, {
  print(head(Sales))
  print(mean(TV))
  print(Radio[Radio < 20])
})
```

These are some of the functions that might be useful
when gathering information about a dataset.
```{r}
length(advertising) # columns!
nrow(advertising)
dim(advertising)
names(advertising)
```

## Simple plots

In exploration, ggplot might be a bit overkill. Faster
plots can be drawn with `plot()` and `pairs()`
```{r}
plot(advertising)
```

A pair-plot gives a bigger picture of the
cross correlations between variables.
The base R `pairs()` function can be tweaked (check `?pairs`)
in order to draw other kind of information of interest.

```{r}
## put histograms on the diagonal
panel_hist <- function(x, ...) {
  usr <- par("usr")
  par(usr = c(usr[1:2], 0, 1.5))
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks
  nb <- length(breaks)
  y <- h$counts
  y <- y / max(y)
  rect(breaks[-nb], 0, breaks[-1], y, col = "cyan", ...)
}
## put (absolute) correlations on the upper panels,
## with size proportional to the correlations.
panel_cor <- function(x, y, digits = 2, prefix = "", cex_cor, ...) {
  par(usr = c(0, 1, 0, 1))
  r <- abs(cor(x, y))
  txt <- format(c(r, 0.123456789), digits = digits)[1]
  txt <- paste0(prefix, txt)
  if (missing(cex_cor)) cex_cor <- 0.8 / strwidth(txt)
  text(0.5, 0.5, txt, cex = cex_cor * r)
}

pairs(advertising,
  upper.panel = panel_cor, diag.panel = panel_hist
)
```

With much less effort, we can obtain a prettier
version of the pair-plot.
```{r}
library(GGally)
ggpairs(advertising, progress = FALSE)
```

Again, with base R let's plot the predictors against the response.
We know how to do that with a single predictor.
```{r}
plot(advertising$TV, advertising$Sales)
```

In case we want to add all plots to a single figure,
we can do it as follow (base R)
```{r}
# set plot parameters
with(advertising, {
  par(mfrow = c(1, 3))
  plot(TV, Sales)
  plot(Radio, Sales)
  plot(Newspaper, Sales)
})
```

adding a smoothed line would look like this
```{r}
# set plot parameters
with(advertising, {
  par(mfrow = c(1, 3))
  scatter.smooth(TV, Sales, col = "red")
  scatter.smooth(Radio, Sales, col = "red")
  scatter.smooth(Newspaper, Sales, col = "red", span = .3) # tweak with span
})
```

The same pictures can also be plotted with `ggplot`,
if preferred (in two ways).

```{r}
library(ggplot2)

ggplot(advertising) +
  geom_point(aes(TV, Sales))
```

First method: draw three separate plots and arrange
them together with `ggarrange()`

```{r}
library(ggpubr)
tvplt <- ggplot(advertising, mapping = aes(TV, Sales)) +
  geom_point() +
  geom_smooth()
radplt <- ggplot(advertising, mapping = aes(Radio, Sales)) +
  geom_point() +
  geom_smooth()
nwsplt <- ggplot(advertising, mapping = aes(Newspaper, Sales)) +
  geom_point() +
  geom_smooth()

ggarrange(tvplt, radplt, nwsplt,
  ncol = 3
)
```

Second method: rearrange the dataset first,
and feed everything to ggplot.

```{r}
library(tibble)
library(tidyr)

## if using melt, need to switch to data.frame
# library(reshape2)
# advertising <- as.data.frame(advertising)
# long_adv <- melt(advertising, id.vars = "Sales")

long_adv <- advertising %>%
  gather(channel, value, -Sales)
head(long_adv)
```
```{r}
ggplot(long_adv, aes(value, Sales)) +
  geom_point() +
  geom_smooth() +
  facet_wrap(~channel, scales = "free")
```

If you're not sure why we need to use the `melt()` function,
check the first lab (Iris dataset), or simply read the `?melt`
helper.

## Simple regression

Check the `lm()` function: first argument is the *formula*.

Let's fit a simple single predictor linear model

```{r}
simple_reg <- lm(Sales ~ TV, data = advertising)
```
Now that we fitted the model, we have access to the
coefficient estimates and we can plot the regression
line.

### Plot

Here two ways of drawing the regression line,
first in base R
```{r}
with(data = advertising, {
  plot(x = TV, y = Sales)
  # abline draws a line from intercept and slope
  abline(
    simple_reg,
    col = "red"
  )
})
```

and in ggplot

```{r}
ggplot(simple_reg, mapping = aes(TV, Sales)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red")
```

Actually, `lm()` does a lot more than just compute
the OLS coefficients.

```{r}
summary(simple_reg)
```

We will analyze in detail this output in the next subsection.


### Summary

**What's in `summary(lm)`?**

- residuals: $Y - X\hat\beta$
- estimate: $\hat\beta$
- std. error: errors on the coefficients $S \sqrt{(X'X)^{-1}_{i+1,i+1}}$
  because $\frac{\hat\beta_i - \beta_i}{S\sqrt{(X'X)^{-1}}}\sim t(n-p)$
- t-value: value of the beta t-statistic
  under the null hypothesis ($H_0: \beta_{i+1} = 0$)
- p-value: probability of the test statistic being beyond t-val

- residual std error: $S = \sqrt{\frac{e'e}{n-p}}$
- multiple R squared: *later*
- adj R squared: *later*

Let's compute them to make sure we understood the concepts

#### Residuals

Easy to retrieve them (actually, 
they are the realization of the residuals)

$$
e = y - x\hat\beta
$$

```{r}
y <- advertising$Sales
x <- cbind(1, advertising$TV)
e <- y - x %*% simple_reg$coefficients
head(tibble(
  lm_res = simple_reg$residuals,
  manual_res = as.vector(e)
))
```

#### Estimate

Estimates are just the $\hat\beta$ values for each predictor (plus intercept).
They are computed with the closed form max likelihood formula.

$$
\hat\beta = (X'X)^{-1}X'Y
$$

```{r}
beta_hat <- solve(t(x) %*% x) %*% t(x) %*% y
head(tibble(
  lm_coeff = simple_reg$coefficients,
  manual_coeff = as.vector(beta_hat)
))
```

#### Standard Error

For each predictor, this represent the variation in the beta estimator. The lower
the standard error is, the higher is the accuracy of that particular coefficient.
For predictor $i$, it's computed as

$$
SE_i = S \sqrt{(X'X)^{-1}_{i+1,i+1}}
$$

where $S$ is the *residuals standard error* (with $S^2$ being the RMS - residual
mean squares), we get the $i+1^{\text{th}}$ element because of the $1$ column for the intercept.

```{r}
n <- nrow(x)
p <- ncol(x)
rms <- t(e) %*% e / (n - p)

# SE for TV
tv_se <- sqrt(rms * solve(t(x) %*% x)[2, 2])
tv_se
```

#### T-value and p-value

These two are related to each other. The first is the
test statistics value, under the null hypothesis
$H_0 : \beta_i = 0$, for the variable

$$
\frac{\hat\beta_i - \beta_i}{S \sqrt{(X'X)^{-1}}}
$$

which is student-T distributed with $n-2$ degrees of freedom.

```{r}
# for TV
t_val <- simple_reg$coefficients[2] / tv_se
t_val
```

And finally, the p-value is simply the probability
on a $t(n-2)$ distribution of the statistic to be
beyond that critical value. Remember that with *beyond*
we mean on both sides of the distribution, since the 
alternative hypothesis $H_1: \beta \neq 0$ is two-sided.

```{r}
# multiply by two because the alternative hyp is two-sided
p_val <- 2 * pt(t_val, n - 2, lower.tail = FALSE)
p_val
```
Here we cannot appreciate the manual computation since the
p-value is very low (meaning that we can reject the null,
favoring the alternative).

Exercise: You can try and compute this value manually on another
simple regression model where we use `Radio` as predictor.

```{r}
radio_simple_reg <- lm(Sales ~ Radio, data = advertising)
summary(radio_simple_reg)
```

