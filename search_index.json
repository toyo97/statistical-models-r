[["index.html", "Statistical Models in R About", " Statistical Models in R Vittorio Zampinetti 2022-10-05 About The book is built upon the course in Statistical Models at Politecnico di Torino, but it is self-contained and thus is accessible by any interested reader. For any feedback, send me an email: . Thank you! "],["introduction.html", "Chapter 1 Introduction 1.1 R and RStudio 1.2 R syntax 1.3 Functions 1.4 Data types 1.5 Data manipulation 1.6 Plotting 1.7 Examples: plot and data manipulation 1.8 Probability 1.9 Extras 1.10 Exercises", " Chapter 1 Introduction 1.1 R and RStudio R: programming language, tailored for statisticians RStudio: development environment, software for editing and running R (and Python) applications It’s going to change name soon, see Posit 1.1.1 Installation First install R programming language (from here), then RStudio environment (from here). 1.2 R syntax Here a brief overview of the main components of the R language. Note: this is not a complete guide on the language nor on programming in general, but rather a quick-start introduction with the most used operations and structures, which assumes some very basic programming skills. For all the rest, Google is your friend (it really is). 1.2.1 Basic operations R can be used as a fully featured calculator, typically directly from the console (see bottom panel in RStudio). Some examples: 2 + 2 # calculator-style ## [1] 4 c(1, 2, 3) * 4 # vector operations ## [1] 4 8 12 t(matrix(c(c(1, 2, 3, 4)), nrow = 2, ncol = 2)) # matrix operations (transpose) ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 As any other programming language, it allows for variables declaration. Variables hold a value which can be assigned, modified, and used for other computations. a &lt;- 2 # assign values to variables b = a + 2 # equal sign also valid, but... b ## [1] 4 The arrow sign &lt;- is the traditional sign for the assignment operation, but also = works. Check this examples to see why &lt;- is recommended. Conditional statements and loops have straightforward syntax (similar to C/C++ , but more compact). if (b == 4 &amp; a &gt; 0) { # if-else statement out &lt;- &quot;&quot; for (i in 1:b) { # for loop out &lt;- paste(out, class(b)) # concatenate two or more strings } print(out) # print string to output } else { stop(&quot;error&quot;) # stop program and exit } ## [1] &quot; numeric numeric numeric numeric&quot; While &lt;- is only used for variable assignment, i.e. filling up the variable allocated space with some value, the = is both used for assignment and function argument passing (as introduced in Functions). 1.3 Functions Functions are called with the () operator and present named arguments. # get Normal samples with 0 (default) mean and 10 standard deviation array &lt;- rnorm(10, sd = 10) # sd is an argument, # the mean is left to the default 0 array ## [1] 13.9451393 -7.1176737 -9.4431784 8.8934239 4.6542653 5.3000924 ## [7] 14.5690151 0.9156836 18.9044585 -6.7012162 You can explicitly call the arguments to which you want to pass the parameters (e.g. sd of the Gaussian). In this case it is necessary, because the second position argument is the mean, but we want it to be the default. 1.3.1 Definition To define a custom function # implements power operation #&#39; #&#39; @param x a real number #&#39; @param n a natural number, defaults to 2 #&#39; @return n-th power of x pow &lt;- function(x, n = 2) { ans &lt;- 1 if (n &gt; 0) { for (i in 1:n) { ans &lt;- ans * x } } else if (n == 0) { # do nothing, ans is already set to 1 } else { stop(&quot;error: n must be non-negative&quot;) } return(ans) } print(paste(&quot;3^5 = &quot;, pow(3, 5), &quot;, 4^2 = &quot;, pow(4), &quot;, 5^0 = &quot;, pow(5, 0))) ## [1] &quot;3^5 = 243 , 4^2 = 16 , 5^0 = 1&quot; The return() call can be omitted (but it’s better not to). pow &lt;- function(a, b) { ans &lt;- 1 # ... # last expression is returned ans # this is equivalent to write return(ans) } 1.4 Data types Every variable in R is an object, which holds a value (or collection of values) and some other attributes, properties or methods. 1.4.1 Base types There are 5 basic data types: numeric (real numbers) integer logical (aka boolean) character complex (complex numbers) 1.4.1.1 Numeric When you write numbers in R, they are going to default to numeric type values a &lt;- 3.4 # decimal b &lt;- -.30 # signed decimal c &lt;- 42 # also without dot, it&#39;s numeric print(paste0(&quot;data types | a:&quot;, class(a), &quot;, b:&quot;, class(b), &quot;, c:&quot;, class(c))) ## [1] &quot;data types | a:numeric, b:numeric, c:numeric&quot; 1.4.1.2 Integer Integer numbers can be enforced typing an L next to the digits. Casting is implicit when the result of an operation involving integers is not an integer int_num &lt;- 50L # 50 stored as integer non_casted_num &lt;- int_num - 2L # result is exactly 48, still int casted_num &lt;- int_num / 7L # implicit cast to numeric type to store decimals print(paste(class(int_num), class(non_casted_num), class(casted_num), sep = &quot;, &quot;)) ## [1] &quot;integer, integer, numeric&quot; 1.4.1.3 Logical The logical type can only hold TRUE or FALSE (in capital letters) bool_a &lt;- FALSE bool_b &lt;- T # T and F are short for TRUE and FALSE bool_a | !bool_b # logical or between F and not-T ## [1] FALSE You can test the value of a boolean also using 0,1 bool_a == 0 # if &#39;A&#39; is not equal to &#39;not B&#39;, raise an error ## [1] TRUE and a sum between logical values, treats FALSE = 0 and TRUE = 1, which is useful when counting true values in logical arrays bool_a + bool_b + bool_b # FALSE + TRUE + TRUE (it&#39;s not an OR operation) ## [1] 2 1.4.1.4 Character A character is any number of characters enclosed in quotes ' ' or double quotes \" \". char_a &lt;- &quot;,&quot; # single character char_b &lt;- &quot;bird&quot; # string char_c &lt;- &#39;word&#39; # single quotes full_char &lt;- paste(char_b, char_a, char_c) # concatenate chars class(full_char) # still a character ## [1] &quot;character&quot; 1.4.1.5 Complex complex_num &lt;- 5 + 4i Mod(complex_num) # try all complex operations, e.g. modulus ## [1] 6.403124 1.4.1.6 Special values NA: “not available”, missing value Inf: infinity NaN: “not-a-number”, undefined value missing_val &lt;- NA is.na(missing_val) # test if value is missing ## [1] TRUE Every operation involving missing values, will output NA missing_val == NA # cannot use == ## [1] NA print(paste( &quot;1/0 = &quot;, 1/0, &quot;, 0/0 = &quot;, 0/0 )) ## [1] &quot;1/0 = Inf , 0/0 = NaN&quot; These special values are not necessarily unwanted, but they require extra care. E.g. Inf can appear also in case of numerical overflow. exp(1000) ## [1] Inf 1.4.1.7 Conversion Variables types can be converted with as.&lt;typename&gt;()-like functions, as long as conversion makes sense. Some examples: v &lt;- T w &lt;- &quot;0&quot; x &lt;- 3.2 y &lt;- 2L z &lt;- &quot;F&quot; cat(paste( paste(x, as.integer(x), sep = &quot; =&gt; &quot;), # from numeric to integer paste(y, as.numeric(y), sep = &quot; =&gt; &quot;), # from integer to numeric paste(y, as.character(y), sep = &quot; =&gt; &quot;), # from integer to character paste(w, as.numeric(w), sep = &quot; =&gt; &quot;), # from number-char to numeric paste(v, as.numeric(v), sep = &quot; =&gt; &quot;), # from logical to numeric sep = &quot;\\n&quot; )) ## 3.2 =&gt; 3 ## 2 =&gt; 2 ## 2 =&gt; 2 ## 0 =&gt; 0 ## TRUE =&gt; 1 as.numeric(z) # from character to numeric (coercion warning - NA) ## Warning: NAs introduced by coercion ## [1] NA 1.4.2 Vectors and matrices 1.4.2.1 Vectors Vectors are build with the c() function. A vector holds values of the same type. vec1 &lt;- c(4, 3, 9, 5, 8) vec1 ## [1] 4 3 9 5 8 Vector operations and aggregation of values is as intuitive as it can be. vec2 &lt;- vec1 - 1 # subtract 1 to all values (broadcast) sum(vec1) # sum all values in vec1 ## [1] 29 mean(vec2) # compute the mean ## [1] 4.8 sort(vec1, decreasing = TRUE) # sort elements in decreasing order ## [1] 9 8 5 4 3 Conversion is still possible and it’s made element by element. char_vec &lt;- as.character(vec1) # convert every value in vec1 to char char_vec ## [1] &quot;4&quot; &quot;3&quot; &quot;9&quot; &quot;5&quot; &quot;8&quot; Range vectors (unit-stepped intervals) are built with start:end syntax. Note: the type of range vectors is integer, not numeric. x_range &lt;- 1:10 class(x_range) ## [1] &quot;integer&quot; They are particularly useful in loops statements: vec3 &lt;- c() # declare an empty vector # iterate all the indices along vec1 for (i in 1:length(vec1)) { vec3[i] &lt;- vec1[i] * i # access with [idx] } vec3 ## [1] 4 6 27 20 40 Vector elements are selected with square brackets []. Putting vectors inside brackets performs slicing vec1[1:3] # first 3 elements ## [1] 4 3 9 vec1[c(1,3)] # only first and third element ## [1] 4 9 vec1[-c(1:3)] # all but elements 1 to 3 ## [1] 5 8 vec1[seq(1, length(vec1), 2)] # odd position elements ## [1] 4 9 8 To find an element in a vector and get its index/indices, the which() function can be used which(vec1 == 3) ## [1] 2 which(vec1 &lt; 5) ## [1] 1 2 And finally, to filter only values that satisfy a certain condition, we can combine which with splicing. vec1[which(vec1 &gt;= 5)] ## [1] 9 5 8 # or, equivalently, using logical masking vec1[vec1 &gt;= 5] ## [1] 9 5 8 1.4.2.2 Matrices Matrices are built with matrix() mat1 &lt;- matrix(1:24, nrow = 6, ncol = 4) mat1 # filled column-wise (default) ## [,1] [,2] [,3] [,4] ## [1,] 1 7 13 19 ## [2,] 2 8 14 20 ## [3,] 3 9 15 21 ## [4,] 4 10 16 22 ## [5,] 5 11 17 23 ## [6,] 6 12 18 24 mat2 &lt;- matrix(1:24, nrow = 6, ncol = 4, byrow = TRUE) mat2 # filled row-wise ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 ## [4,] 13 14 15 16 ## [5,] 17 18 19 20 ## [6,] 21 22 23 24 dim(mat2) # get dimensions ## [1] 6 4 c(nrow(mat2), ncol(mat2)) # get number of rows and cols separately ## [1] 6 4 # or, equivalently dim(mat2)[1] # nrow ## [1] 6 All indexing operations available on vectors, are also available on matrices mat2[1, 1] # element 1,1 ## [1] 1 mat2[3, ] # third row (empty space for all elements) ## [1] 9 10 11 12 mat2[1:2, 1:2] # upper left 2x2 sub-matrix ## [,1] [,2] ## [1,] 1 2 ## [2,] 5 6 t(mat2) # transposed matrix ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 5 9 13 17 21 ## [2,] 2 6 10 14 18 22 ## [3,] 3 7 11 15 19 23 ## [4,] 4 8 12 16 20 24 Operations with matrix and vectors can be both element-wise and matrix operations (e.g. scalar product). Note that a vector built with c() is a column vector by default. Some examples: diagonal_mat &lt;- diag(nrow = 4) # 4x4 identity matrix # element by element diagonal_mat * 1:2 # note: 1:2 is repeated to match the matrix dimensions ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] 0 2 0 0 ## [3,] 0 0 1 0 ## [4,] 0 0 0 2 diagonal_mat %*% seq(2, 8, 2) # matrix multiplication (4,4) x (4, 1) -&gt; (4, 1) ## [,1] ## [1,] 2 ## [2,] 4 ## [3,] 6 ## [4,] 8 v1 &lt;- 1:4 v2 &lt;- 4:1 v1 %*% v2 # here v1 is implicitly converted to row vector ## [,1] ## [1,] 20 1.4.2.3 Arrays Arrays are multi-dimensional vectors (generalization of a matrix with more than two dimensions). They work pretty much like matrices. arr1 &lt;- array(1:24, dim = c(2, 4, 3)) arr1 ## , , 1 ## ## [,1] [,2] [,3] [,4] ## [1,] 1 3 5 7 ## [2,] 2 4 6 8 ## ## , , 2 ## ## [,1] [,2] [,3] [,4] ## [1,] 9 11 13 15 ## [2,] 10 12 14 16 ## ## , , 3 ## ## [,1] [,2] [,3] [,4] ## [1,] 17 19 21 23 ## [2,] 18 20 22 24 arr1[2, 1, 3] # get one element ## [1] 18 sliced_arr &lt;- arr1[, 2, ] # slice at column 2 sliced_arr ## [,1] [,2] [,3] ## [1,] 3 11 19 ## [2,] 4 12 20 dim(sliced_arr) # reduces ndims by one (dimension selected is dropped) ## [1] 2 3 1.4.3 Lists and dataframes Lists are containers that can hold different data types. Each entry, which can even be another list, has a position in the list and can also be named. list1 &lt;- list(1:3, TRUE, x = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) list1 ## [[1]] ## [1] 1 2 3 ## ## [[2]] ## [1] TRUE ## ## $x ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; list1[[3]] # access with through index ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; list1$x # access through name ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; Dataframes are collections of columns that have the same length. Contrarily to matrices, columns in dataframes can be of different types. They are the most common way of representing structured data and most of the dataset will be stored in dataframes. df1 &lt;- data.frame(x = 1, y = 1:10, char = sample(c(&quot;a&quot;, &quot;b&quot;), 10, replace = TRUE)) df1 # x was set to just one value and gets repeated (&#39;recycled&#39;) ## x y char ## 1 1 1 b ## 2 1 2 a ## 3 1 3 a ## 4 1 4 a ## 5 1 5 b ## 6 1 6 a ## 7 1 7 a ## 8 1 8 a ## 9 1 9 a ## 10 1 10 b df1[[2]] # access through column index ## [1] 1 2 3 4 5 6 7 8 9 10 df1$x # access through column name ## [1] 1 1 1 1 1 1 1 1 1 1 df1[, 3] # access with matrix-style index ## [1] &quot;b&quot; &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;b&quot; &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;b&quot; df1[2:4, ] # can also select subset of rows ## x y char ## 2 1 2 a ## 3 1 3 a ## 4 1 4 a The dplyr library provides another dataframe object (called tibble) which has all the effective features of Base R data.frame and none of the deprecated functionalities. It’s simply a newer version of dataframes (therefore recommended over the old one). library(&quot;tibble&quot;) tibble(x = 1:15, y = 1, z = x / y) # tibble dataframe ## # A tibble: 15 × 3 ## x y z ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 ## 2 2 1 2 ## 3 3 1 3 ## 4 4 1 4 ## 5 5 1 5 ## 6 6 1 6 ## 7 7 1 7 ## 8 8 1 8 ## 9 9 1 9 ## 10 10 1 10 ## 11 11 1 11 ## 12 12 1 12 ## 13 13 1 13 ## 14 14 1 14 ## 15 15 1 15 For more information on tibble and its advantages with respect to traditional dataframes, type vignette(\"tibble\") in an R console. Notice that you can convert datasets to tibble with as_tibble(), while with as.data.frame() you will get a Base R dataframe. 1.5 Data manipulation Now that we know what a dataframe is and how it is generated, we can focus on data manipulation. The dplyr library provides an intuitive way of working with datasets. For instance, let’s consider the mtcars dataset. library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union mtcars$modelname &lt;- rownames(mtcars) # name column with models mtcars &lt;- as_tibble(mtcars) # convert to tibble mtcars # display the raw data ## # A tibble: 32 × 12 ## mpg cyl disp hp drat wt qsec vs am gear carb modelname ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 21 6 160 110 3.9 2.62 16.5 0 1 4 4 Mazda RX4 ## 2 21 6 160 110 3.9 2.88 17.0 0 1 4 4 Mazda RX4 … ## 3 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 Datsun 710 ## 4 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 Hornet 4 D… ## 5 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 Hornet Spo… ## 6 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 Valiant ## 7 14.3 8 360 245 3.21 3.57 15.8 0 0 3 4 Duster 360 ## 8 24.4 4 147. 62 3.69 3.19 20 1 0 4 2 Merc 240D ## 9 22.8 4 141. 95 3.92 3.15 22.9 1 0 4 2 Merc 230 ## 10 19.2 6 168. 123 3.92 3.44 18.3 1 0 4 4 Merc 280 ## # … with 22 more rows Let’s say we want to get the cars with more than 100 hp, and we are just interested in the car model name and we want the data to be sorted in alphabetic order. mtcars %&gt;% # send the data into the transformation pipe dplyr::filter(hp &gt; 100) %&gt;% # filter rows with hp &gt; 100 dplyr::select(modelname) %&gt;% # filter columns (select only modelname col) dplyr::arrange(modelname) # display in alphabetic order ## # A tibble: 23 × 1 ## modelname ## &lt;chr&gt; ## 1 AMC Javelin ## 2 Cadillac Fleetwood ## 3 Camaro Z28 ## 4 Chrysler Imperial ## 5 Dodge Challenger ## 6 Duster 360 ## 7 Ferrari Dino ## 8 Ford Pantera L ## 9 Hornet 4 Drive ## 10 Hornet Sportabout ## # … with 13 more rows There are many other dplyr functions for data transformation. This useful cheatsheet summarizes most of them for quick access. 1.6 Plotting In base R plots are built with several calls to functions, each of which edit the current canvas. For instance, to plot some points and a line: # generate synthetic data n_points &lt;- 20 x = 1:n_points y = 3 * x + 2 * rnorm(n_points) plot(x, y) abline(a = 0, b = 3, col = &#39;red&#39;) However, the ggplot library is now the new standard plotting library. In ggplot, a plot is decomposed in three main components: data, coordinate system and visual marks, called geoms. The plot is built by stacking up layers of visualization objects. Data is in form of dataframes and the columns are selected in the aesthetics arguments. The same plot shown before can be drawn with ggplot in the following way. library(ggplot2) gg_df &lt;- tibble(x = x, y = y) ggplot(gg_df) + geom_point(mapping = aes(x, y)) + geom_abline(mapping = aes(intercept = 0, slope = 3), color = &quot;red&quot;) This is just a brief example. More will be seen in the next lessons. Check out this cheatsheet for quick look-up on ggplot functions. 1.7 Examples: plot and data manipulation Combining altogether, here a data visualization workflow on the Gapminder dataset. library(gapminder) # have a quick look at the Gapminder dataset str(gapminder) ## tibble [1,704 × 6] (S3: tbl_df/tbl/data.frame) ## $ country : Factor w/ 142 levels &quot;Afghanistan&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ continent: Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... ## $ year : int [1:1704] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ... ## $ lifeExp : num [1:1704] 28.8 30.3 32 34 36.1 ... ## $ pop : int [1:1704] 8425333 9240934 10267083 11537966 13079460 14880372 12881816 13867957 16317921 22227415 ... ## $ gdpPercap: num [1:1704] 779 821 853 836 740 ... A factor, which we haven’t seen yet, is just a data-type characterizing a discrete categorical variable; the levels of a factor describe how many distinct categories it can take value from (e.g. the variable continent takes values from the set {Africa, Americas, Asia, Europe, Oceania}). Let’s say we want to compare the GDP per capita of some different countries (Italy, Japan, Brasil and Ethiopia), plotted against time (year by year). # transform the dataset according to what is necessary wanted_countries &lt;- c(&quot;Italy&quot;, &quot;Japan&quot;, &quot;Brazil&quot;, &quot;Ethiopia&quot;) gapminder %&gt;% dplyr::filter(country %in% wanted_countries) %&gt;% # now feed the filtered data to ggplot (using the pipe op) ggplot() + geom_line(aes(year, gdpPercap, color = country)) If we want to add some information about the same measure over the whole continent, showing for instance the boundaries of GDP among all countries in the same continent of the four selected countries, this is more or less what we can do # give all the data to ggplot, we&#39;ll filter later gapminder %&gt;% ggplot() + geom_line(data = . %&gt;% dplyr::filter(country %in% wanted_countries), aes(year, gdpPercap, color = country)) + # now group by continent and get the upper/lower bounds geom_ribbon(data = . %&gt;% dplyr::filter(!is.na(gdpPercap)) %&gt;% # min(NA) = NA, make sure NAs are excluded dplyr::group_by(continent, year) %&gt;% # gather all entries for each continent separately dplyr::summarize(minGdp = min(gdpPercap), maxGdp = max(gdpPercap), across()) %&gt;% # compute aggregated quantity (min/max) dplyr::filter(country %in% wanted_countries), aes(ymin = minGdp, ymax = maxGdp, x = year, color = country, fill = country), alpha = 0.1, linetype = &quot;dashed&quot;, size = 0.2) -&gt; plt plt ## `summarise()` has grouped output by &#39;continent&#39;, &#39;year&#39;. You can override using ## the `.groups` argument. But, since it looks a bit confusing, we might want four separate plots. gapminder %&gt;% dplyr::filter(country %in% wanted_countries) %&gt;% dplyr::pull(continent) %&gt;% # extract one column from a dataframe (different from select) unique() -&gt; wanted_continents gapminder %&gt;% dplyr::filter(continent %in% wanted_continents) %&gt;% ggplot() + geom_line(data = . %&gt;% dplyr::filter(country %in% wanted_countries), aes(year, gdpPercap, color = country)) + # now group by continent and get the upper/lower bounds geom_ribbon(data = . %&gt;% dplyr::filter(!is.na(gdpPercap)) %&gt;% # min(NA) = NA, make sure NAs are excluded dplyr::group_by(continent, year) %&gt;% # gather all entries for each continent separately dplyr::summarize(minGdp = min(gdpPercap), maxGdp = max(gdpPercap), across()) %&gt;% # compute aggregated quantity (min/max) dplyr::filter(country %in% wanted_countries), aes(ymin = minGdp, ymax = maxGdp, x = year, color = country, fill = country), alpha = 0.1, linetype = &quot;dashed&quot;, size = 0.2) + facet_wrap(vars(continent)) + labs(title = paste(&quot;Country GDP per capita compared with&quot;, &quot;continent lower and upper bounds&quot;), x = &quot;year&quot;, y = &quot;GDP per capita (PPP dollars)&quot;) ## `summarise()` has grouped output by &#39;continent&#39;, &#39;year&#39;. You can override using ## the `.groups` argument. 1.8 Probability Base R provide functions to handle almost any probability distribution. These functions are usually divided into four categories: density function distribution function quantile function random function (sampling) n &lt;- 10 normal_samples &lt;- rnorm(n = n, mean = 0, sd = 1) # sample 10 Gaussian samples normal_samples ## [1] -1.116223972 -1.100568677 -1.130466838 0.001286327 -0.601195239 ## [6] -0.483271908 0.312010652 -0.090273060 2.202108137 -0.769163806 dnorm(normal_samples, mean = 2, sd = 1) # compute the density function (over another Normal) ## [1] 0.003106171 0.003261065 0.002971020 0.054130001 0.013540814 0.018274325 ## [7] 0.095982197 0.044889145 0.390876986 0.008625153 pnorm(normal_samples, mean = 0, sd = 1) # cumulative distribution function ## [1] 0.1321631 0.1355422 0.1291398 0.5005132 0.2738550 0.3144513 0.6224838 ## [8] 0.4640351 0.9861712 0.2208980 qnorm(c(0.05, 0.95), mean = 0, sd = 1) # get the quantiles of a normal ## [1] -1.644854 1.644854 1.9 Extras 1.9.1 File system and helper R language provides several tools for management of files and function help. Here some useful console commands. Note that most of them are also available on RStudio through the graphic interface (buttons). R saves all the variables and you can display them with ls(). rm(list = ls()) # clear up the space removing all variables stored so far # let&#39;s add some variables x &lt;- 1:10 y &lt;- x[x %% 2 == 0] ls() # check variables in the environment ## [1] &quot;x&quot; &quot;y&quot; The working directory is the folder located on your computer from which R navigates the filesystem. getwd() # check your wd ## [1] &quot;/Users/zemp/phd/teach_ms/statistical-models-r&quot; setwd(&quot;./tmp&quot;) # set the working directory to an arbitrary (existing) folder # save the current environment save.image(&quot;./01_test.RData&quot;) # check that it&#39;s on the working directory dir() RStudio typically save the environment automatically, but sometimes (if not every time you close R) you should clear the environment variables, because loading many variables when opening RStudio might fill up too much memory. You can also read function helpers simply by typing ?function_name. This will open a formatted page with information about a specific R function or object. ?quit # help for the quit function ?Arithmetic # help for more general syntax information help(Trig) # or use help(name) 1.9.2 Packages Packages can be installed via command line using install.packages(\"package_name\"), or through RStudio graphical interface. # the following function call is commented because package installation should # not be included in a script (but you can find it commented, showing that the # script requires a package as dependency) # install.packages(&quot;tidyverse&quot;) And then you can load the package with library. library(tidyverse) ## ── Attaching packages ─────────────────────────────────────────────────────────────── tidyverse 1.3.2 ── ## ✔ tidyr 1.2.1 ✔ stringr 1.4.1 ## ✔ readr 2.1.2 ✔ forcats 0.5.2 ## ✔ purrr 0.3.4 ## ── Conflicts ────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 1.9.3 Arrow sign The difference between &lt;- and = is not just programming style preference. Here an example where using = rather than &lt;- makes a difference: # gives error: argument &#39;b&#39; is not part of &#39;within&#39; within(data.frame(a= rnorm(2)), b = a^2) ## Error in eval(substitute(expr), e): argument is missing, with no default # &#39;b&lt;-a^2&#39; is the value passed to the expr argument of within() within(data.frame(a = rnorm(2)), b &lt;- a^2) ## a b ## 1 -0.006336495 4.015117e-05 ## 2 -0.894681269 8.004546e-01 Although this event might never occur in one’s programming experience, it’s safer (and more elegant) to use &lt;- when assigning variable. Besides, -&gt; is also valid and it is used (more intuitive) when assigning pipes result to variables. library(dplyr) starwars %&gt;% dplyr::mutate(bmi = mass / ((height / 100) ^ 2)) %&gt;% dplyr::filter(!is.na(bmi)) %&gt;% dplyr::group_by(species) %&gt;% dplyr::summarise(bmi = mean(bmi)) %&gt;% dplyr::arrange(desc(bmi)) -&gt; x x ## # A tibble: 32 × 2 ## species bmi ## &lt;chr&gt; &lt;dbl&gt; ## 1 Hutt 443. ## 2 Vulptereen 50.9 ## 3 Yoda&#39;s species 39.0 ## 4 Kaleesh 34.1 ## 5 Droid 32.7 ## 6 Dug 31.9 ## 7 Trandoshan 31.3 ## 8 Sullustan 26.6 ## 9 Zabrak 26.1 ## 10 Besalisk 26.0 ## # … with 22 more rows 1.10 Exercises LogSumExp trick Try to implement the log-sum-exp trick in a function that takes as argument three numeric variables and computed the log of the sum of the exponentials in a numerically stable way. See this Wiki paragraph if you don’t know the trick yet. log_sum_exp3 &lt;- function(a, b, c) { # delete this function and re-write it using the trick # in order to make it work return(log(exp(a) + exp(b) + exp(c))) } # test - this result is obviously wrong: edit the function above log_sum_exp3(a = -1000, b = -1001, c = -999) # should give -998.5924 ## [1] -Inf "],["some-elementary-statistics-problems.html", "Chapter 2 Some elementary statistics problems 2.1 Hypothesis testing 2.2 Confidence intervals 2.3 P-value", " Chapter 2 Some elementary statistics problems In the following paragraphs, we will see some applications of the R software to solve elementary statistics problems. 2.1 Hypothesis testing 2.1.1 Example The rector of Politecnico di Torino wants to monitor the spread of Covid-19 virus inside the university, to check whether it is in line with the overall diffusion on the Italian territory, or whether it is higher, and therefore requires extra safety measures. Tests are made on a sample of \\(n = 250\\) students out of the total \\(N = 5000\\) students (because of whatever reasons, such as economic or sustainability concerns). We assume that the tests are perfect, meaning that they have \\(100\\%\\) sensitivity (true positive rate) and specificity (true negative rate). We know that \\(p_0 = 0.015\\) is the prevalence of the virus in the Italian population at the end of 2021, and we are given a warning threshold for the prevalence of \\(p_1 = 0.04\\), above which the rector will have reasons to adopt more restrictive measures. 2.1.2 False alarm Question: given a threshold \\(x_0\\) of positive tests, what are the false alarm and the false non-alarm probabilities? Formally, a false alarm (type I error) probability is defined as the probability of the r.v. counting the positive tests \\(X\\) being greater than or equal to the threshold \\(x_0\\), conditioned on the fact that the prevalence is not different than the Italian reference value \\(p_0\\). I.e. \\[ P(X \\geq x_0 | p = p_0) \\] Recall that \\(X \\sim \\text{Hypergeometric}(N, K, n)\\) where \\(p = K/N\\). The pmf is the following \\[ P(X = x) = \\frac{\\binom{K}{x}\\binom{N - K}{n - x}}{\\binom{N}{n}}\\,, \\] which leads to the false alarm probability computation \\[ P(X \\geq x_0 | p = p_0) = \\sum_{k = x_0}^{n}P(X = k) = \\sum_{k = x_0}^{n}\\frac{\\binom{K}{k} \\binom{N - K}{n - k}}{\\binom{N}{n}}\\,. \\] When \\(N\\) is large enough and \\(p\\) small enough, we can use the Binomial approximation. To be clear, with such approximation, we assume that while sampling from the population, the proportion of positives stays the same (which in our case it is safe to assume). Therefore we approximate the false alarm probability with \\[ P(X \\geq x_0 | p = p_0) = \\alpha \\approx \\sum_{k = x_0}^{n}{\\binom{n}{k}}p^{k}(1-p)^{n-k}\\,. \\] Similarly, we compute the false non-alarm probability (type II error), defined as the probability of the r.v. counting the positive tests \\(X\\) being lower than the threshold \\(x_0\\), conditioned on the fact that the prevalence is the one we identified as worrying, i.e. \\(p = p_1\\). \\[ P(X &lt; x_0 | p = p_1) = \\beta \\approx \\sum_{k = 0}^{x_0 - 1}{\\binom{n}{k}}p^{k}(1-p)^{n-k}\\,. \\] Let’s now compute these quantities using R, for several values of \\(x_0\\). # import the main libraries library(tidyverse) # set random seed for reproducibility set.seed(42) # define the parameters of the test N &lt;- 5000 n &lt;- 250 p0 &lt;- .015 p1 &lt;- .04 K0 &lt;- floor(N * p0) # get the lower integer part in case the product is not natural K1 &lt;- floor(N * p1) # false positive (check phyper params with ?phyper) # we subtract 1 to x0 because we are looking for P(X &gt;= x0) = 1 - P(X &lt; x0-1), not P(X &gt; x0) fp_df &lt;- tibble(x0 = 1:10, hyp = 1 - phyper(x0 - 1, K0, N - K0, n), bin = 1 - pbinom(x0 - 1, n, p0)) # let&#39;s view the result fp_df ## # A tibble: 10 × 3 ## x0 hyp bin ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.979 0.977 ## 2 2 0.896 0.890 ## 3 3 0.732 0.725 ## 4 4 0.521 0.517 ## 5 5 0.321 0.322 ## 6 6 0.171 0.176 ## 7 7 0.0795 0.0847 ## 8 8 0.0325 0.0364 ## 9 9 0.0118 0.0141 ## 10 10 0.00382 0.00494 # false negative # again, we subtract 1: P(X &lt; x0) = P(X &lt;= x0-1) and pbinom(x, n, p) := P(X &lt;= x; n, p) fn_df &lt;- tibble(x0 = 1:10, hyp = phyper(x0 - 1, K1, N - K1, n), bin = pbinom(x0 - 1, n, p1)) fn_df ## # A tibble: 10 × 3 ## x0 hyp bin ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.0000283 0.0000370 ## 2 2 0.000339 0.000422 ## 3 3 0.00203 0.00242 ## 4 4 0.00810 0.00930 ## 5 5 0.0243 0.0270 ## 6 6 0.0587 0.0633 ## 7 7 0.119 0.125 ## 8 8 0.208 0.215 ## 9 9 0.322 0.328 ## 10 10 0.452 0.455 We observe that with \\(x_0 = 8\\) we have \\[ \\alpha = 0.033, \\beta = 0.208 \\] but in order to get a more complete overview, we could plot those values together library(scales) # to define custom scale on axes # join the two dataframes tot_df &lt;- left_join(fp_df, fn_df, by = &quot;x0&quot;, suffix = c(&quot;alpha&quot;, &quot;beta&quot;)) tot_df %&gt;% ggplot() + geom_bar(aes(x0, hypalpha, fill = &quot;alpha&quot;), alpha = .3, stat = &quot;identity&quot;) + geom_bar(aes(x0, hypbeta, fill = &quot;beta&quot;), alpha = .3, stat = &quot;identity&quot;) + geom_smooth(aes(x0, binalpha, color = &quot;alpha&quot;), se = FALSE) + geom_smooth(aes(x0, binbeta, color = &quot;beta&quot;), se = FALSE) + labs(x = &quot;thresh&quot;, y = &quot;prob&quot;, fill = &quot;hyper&quot;, color = &quot;binom&quot;) + scale_x_continuous(breaks = pretty_breaks(10)) # print integers on the x axis ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; The plot suggests that a good compromise could also be \\(x_0 = 7\\), but there is no rule in choosing such threshold, and in fact, we might be more interested in a safer threshold in terms of lower false alarm probability, or lower false non-alarm probability. This decision process can be thought as two simple hypotheses test: we consider the null hypothesis \\(H_0: p = p_0\\). Given our observation, i.e. \\(x\\), count of positive tests, realization of the \\(X\\) r.v. if \\(x &lt; x_0\\) we have no reasons to reject \\(H_0\\) if \\(x \\geq x_0\\) we reject \\(H_0\\) and accept the alternative hypothesis 2.1.3 The Binomial approximation Exercise: The plot above shows that the Binomial distribution offers indeed a good approximation of the Hypergeometric. However, as an exercise, we can repeat the experiment changing the values of \\(N\\) and \\(p\\). repeat the computations above tweaking the parameters and observe the resulting plot. With which parameters do you observe a poor approximation of the Hypergeometric with the Binomial distribution? Feel free to plot different quantities, such as the density instead of the cumulative distribution 2.2 Confidence intervals In case we have two samples \\(\\mathbf{X} = X_1, ..., X_m\\) and \\(\\mathbf{Y} = Y_1, ..., Y_n\\) coming from two unknown Normal distributions, we might want to test whether they come from distributions with same mean, or in other words, whether they are both scattered around a common value. For example, we could be interested in checking whether a set of jewelry items, which have some variability in weight, has been produced in the same original factory or if it is counterfeit. After defining a confidence level \\(1-\\alpha\\) and a variable that we want to limit with some “confidence”, we can compute the confidence interval \\((\\Delta_l, \\Delta_u)\\). The confidence interval tells us where the real value of the variable of interest can fall, with some confidence given the evidence. The higher is the confidence that we want to enforce, the larger the interval will be, but remember that a wide confidence interval is often not useful. 2.2.1 Example Let’s start with a simple example, where we have two normal samples, of which we know nothing (not the mean, nor the variance): \\[ X_1, ..., X_m \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal N (\\mu_x, \\sigma_x^2)\\,,\\quad Y_1, ..., Y_n \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal N (\\mu_y, \\sigma_y^2) \\] The goal is to get a confidence interval for the variable \\(\\mu_x - \\mu_y\\), therefore an interval in which we can expect the two means difference to be with confidence \\(1 - \\alpha\\). Assuming equal variance in the two samples and both \\(n, m\\) sufficiently large, the CI is computed as \\[ \\bar{\\mathbf{x}} - \\bar{\\mathbf{y}} \\pm z_{\\alpha/2}\\sqrt{\\frac{s_x^2}{m} + \\frac{s_y^2}{n}}\\,. \\] Let’s simulate some data and use R to get the confidence interval, using as paramters \\(\\mu_x = 10, \\sigma_x^2 = 36\\) and \\(\\mu_y = 10, \\sigma_y^2 = 49\\). We draw \\(m = 100\\) and \\(n = 120\\) samples respectively. # set the parameters m &lt;- 100 mu_x &lt;- 10 sigma_x &lt;- 6 n &lt;- 120 mu_y &lt;- 10 sigma_y &lt;- 7 # use rnorm to draw independent and identically distributed samples x &lt;- rnorm(m, mu_x, sigma_x) y &lt;- rnorm(n, mu_y, sigma_y) At this point we can define the confidence level and compute the CI. alpha &lt;- 0.05 # typical value for alpha # define a custom function based on the formula above confidence_interval &lt;- function(x, y, alpha) { # note that mean() is the sample mean dev &lt;- mean(x) - mean(y) # var() is the sample variance, not the actual variance delta &lt;- qnorm(1 - alpha/2) * sqrt(var(x) / m + var(y) / n) return(c(dev - delta, dev + delta)) } confidence_interval(x, y, alpha) ## [1] -0.859210 2.540521 We notice that the interval contains the value 0, and this is due to the fact that both samples have same mean, therefore the sample means will also be, with enough samples, very similar, thus with zero (or close to zero) difference. 2.2.2 The four cases To better clarify, we can divide the confidence intervals over two samples in four cases: we know the variances of the two samples, \\(\\sigma_x^2, \\sigma_y^2\\), we don’t know the variances, but the samples sizes are large enough, we don’t know the variances and we cannot assume large sample size, but we assume homoscedasticity (equal variances), we don’t know the variances, samples size is not large and homoscedasticity cannot be assumed. 2.2.2.1 Case 1 The variable of interest is, just like in any of the following cases, \\(\\mu_x - \\mu_y\\), which we don’t know. What we do know, given normality of the samples and from some background theory, is that \\[ \\overline{\\mathbf X} - \\overline{\\mathbf Y}\\sim \\mathcal N(\\mu_x - \\mu_y, \\frac{\\sigma_x^2}{m} + \\frac{\\sigma_y^2}{n})\\,. \\] Normalizing, we get \\[ \\frac{\\overline{\\mathbf X} - \\overline{\\mathbf Y} - (\\mu_x - \\mu_y)}{\\sqrt{\\frac{\\sigma_x^2}{m} + \\frac{\\sigma_y^2}{n}}}\\sim \\mathcal N(0, 1)\\,, \\] and thus, to get a confidence interval, we simply write the coverage probability \\[ P(- z_{\\alpha/2} &lt; \\frac{\\overline{\\mathbf X} - \\overline{\\mathbf Y} - (\\mu_x - \\mu_y)}{\\sqrt{\\frac{\\sigma_x^2}{m} + \\frac{\\sigma_y^2}{n}}} &lt; z_{\\alpha/2} ; \\mu_x, \\mu_y) \\equiv 1 - \\alpha \\] and rearranging the terms we find the confidence interval for \\(\\mu_x - \\mu_y\\) in case 1: \\[ \\left( \\overline{\\mathbf X} - \\overline{\\mathbf Y} - z_{\\alpha/2} \\sqrt{\\frac{\\sigma_x^2}{m} + \\frac{\\sigma_y^2}{n}}, \\overline{\\mathbf X} - \\overline{\\mathbf Y} + z_{\\alpha/2} \\sqrt{\\frac{\\sigma_x^2}{m} + \\frac{\\sigma_y^2}{n}}\\right) \\] 2.2.2.2 Case 2 If \\(m, n\\) are “large”, then the sample variance will asymptotically get closer to the variance. In the previous example we assumed large sample size, that’s why we simply replaced \\(\\sigma\\) with \\(s\\) in the above formula. The interval \\[ \\bar{\\mathbf{x}} - \\bar{\\mathbf{y}} \\pm z_{\\alpha/2}\\sqrt{\\frac{s_x^2}{m} + \\frac{s_y^2}{n}}\\,. \\] is a confidence interval for \\(\\mu_x - \\mu_y\\) with approximately level \\(1-\\alpha\\). 2.2.2.3 Case 3 In case we assume homoscedasticity, we leverage on the observation as much as we can, therefore we compute a weighted average (pooled) of the sample variances and we use that estimator to approximate the common variance. \\[ s_p^2 = \\frac{(m - 1) s_x^2 + (n - 1) s_y^2}{m + n - 2}\\,. \\] One can prove that the r.v \\(\\frac{(m + n - 2) s_p^2}{\\sigma^2} \\sim \\chi^2(m + n - 2)\\) and from that follows \\[ \\frac{\\overline{\\mathbf X} - \\overline{\\mathbf Y} - (\\mu_x - \\mu_y)}{\\sqrt{s_p^2 \\left( \\frac{1}{m} + \\frac{1}{n}\\right)}} \\sim t(m + n - 2)\\,, \\] Where \\(t(d)\\) is a T-student distribution with \\(d\\) degrees of freedom and, compared to a standard Normal distribution, has larger tails, which means that the quantiles over small values such as \\(\\alpha\\) will be higher wrt the std normal. x_dt &lt;- seq(- 5, 5, by = 0.01) norm_t_df &lt;- tibble(x = x_dt, p_dt = dt(x, df = 3), p_nt = dnorm(x)) norm_t_df %&gt;% ggplot() + geom_line(aes(x, p_dt, color = &quot;t-stud&quot;)) + geom_line(aes(x, p_nt, color = &quot;norm&quot;)) + labs(title = &quot;T-student with df = 3 and Std Normal comparison&quot;, x = &quot;x&quot;, y = &quot;p(x)&quot;) Following the same approach of the previous cases, the confidence interval is found using the quantiles of the specific T-student distributions. Before we computed the CI “manually”, but luckily there exists a function which already implements this formula (and not only this): # set var.equal = TRUE for pooled variance test_obj &lt;- t.test(x, y, conf.level = 1 - alpha, var.equal = TRUE) test_obj # outputs some information ## ## Two Sample t-test ## ## data: x and y ## t = 0.9646, df = 218, p-value = 0.3358 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.8770073 2.5583183 ## sample estimates: ## mean of x mean of y ## 10.195089 9.354433 test_obj$conf.int # we&#39;re interested in the confidence interval ## [1] -0.8770073 2.5583183 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 2.2.2.4 Case 4 Otherwise, in case we cannot assume that the two samples have the same variance. In R this specific confidence interval can be computed setting the var.equal flag to false in t.test(). # or leaving it to default = F t.test(x, y, var.equal = FALSE) # conf.level = 0.95 by default ## ## Welch Two Sample t-test ## ## data: x and y ## t = 0.96929, df = 214.36, p-value = 0.3335 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.8688616 2.5501726 ## sample estimates: ## mean of x mean of y ## 10.195089 9.354433 This is called the Welch-Satterthwaite approximation. 2.3 P-value In the context of hypothesis testing, the p-value is the probability, under the null hypothesis, that we obtain a test statistic at least as contradictory to \\(H_0\\) as the statistic from the available sample. In other words, it’s a measure of the null hypothesis support in a 0 to 1 range. 2.3.1 Example In a fair French roulette, the red probability is 18/37. we observe 20 reds out of 30. Compute the p-value of the null hypothesis of the roulette being fair, with respect to the alternative hypothesis of the roulette being skewed towards the red, in the same situation, compute the p-value of the null hypothesis of the roulette being fair wrt the alternative hypothesis of it not being fair, repeat a. and b. with 200 reds out of 380 as outcome. Given \\(X = \\#\\text{reds}\\), we can compute the following probability under the null hypothesis, noticing that \\(X \\sim \\text{Binomial}(n, p)\\) with \\(n = 30\\): \\[ P(X \\geq 20 | H_0) = \\sum_{k = 20}^{30} {30 \\choose k} \\left(\\frac{18}{37}\\right)^k\\left(\\frac{19}{37}\\right)^{(30-k)} \\] Note that we specify the condition \\(\\geq 20\\) as the p-value stands for the probability of observing an outcome at least as contradictory as the one observed. In R, we can compute it “by hand”: p0 = 18/37 n = 30 # P(X &gt; 19) \\equiv P(X &gt;= 20) pbinom(19, n, p0, lower.tail = FALSE) # notice the lower.tail flag ## [1] 0.03598703 or use the Binomial test function # check how it works ?binom.test bin_test &lt;- binom.test(20, n, p0, alternative = &quot;greater&quot;) bin_test ## ## Exact binomial test ## ## data: 20 and n ## number of successes = 20, number of trials = 30, p-value = 0.03599 ## alternative hypothesis: true probability of success is greater than 0.4864865 ## 95 percent confidence interval: ## 0.5005613 1.0000000 ## sample estimates: ## probability of success ## 0.6666667 bin_test$p.value ## [1] 0.03598703 # sketch of &quot;greater&quot; type p-value sketch_df &lt;- tibble(x = 0:n, px = dbinom(x, n, p0)) sketch_df %&gt;% ggplot() + geom_point(aes(x, px)) + geom_segment(data = . %&gt;% dplyr::filter(x &gt;= 20), aes(x = x, xend = x, y = 0, yend = px), color = 2) If the alternative hypothesis is just \\(H_1: p \\neq 18/37\\) then the contradiction can happen both on the left and on the right side of the curve, i.e. the roulette favors the red or the roulette favors the black. Therefore we have to sum the two probabilities: \\[ P(X \\geq 20 | H_0) + P(X &lt; 10 | H_0)\\,. \\] We compute this quantity first using pbinom as before pbinom(19, n, p0, lower.tail = FALSE) + pbinom(9, n, p0) ## [1] 0.06614782 and then using the test function bin_test_2 &lt;- binom.test(20, n, p0, alternative = &quot;two.sided&quot;) bin_test_2 ## ## Exact binomial test ## ## data: 20 and n ## number of successes = 20, number of trials = 30, p-value = 0.06615 ## alternative hypothesis: true probability of success is not equal to 0.4864865 ## 95 percent confidence interval: ## 0.4718800 0.8271258 ## sample estimates: ## probability of success ## 0.6666667 bin_test_2$p.value ## [1] 0.06614782 # sketch of bilateral type p-value sketch_df &lt;- tibble(x = 0:n, px = dbinom(x, n, p0)) sketch_df %&gt;% ggplot() + geom_point(aes(x, px)) + geom_segment(data = . %&gt;% dplyr::filter(x &gt;= 20), aes(x = x, xend = x, y = 0, yend = px), color = 2) + geom_segment(data = . %&gt;% dplyr::filter(x &lt; 10), aes(x = x, xend = x, y = 0, yend = px), color = 3) When \\(n\\) increases, and we can consider it large enough, we can use the normal approximation of the binomial (for the CLT), i.e. \\[ Z = \\frac{X - np}{\\sqrt{np(1-p)}} \\stackrel{n\\rightarrow +\\infty}{\\approx} \\mathcal N(0,1)\\,. \\] Under the null hypothesis, in the unilateral case, we compute the p-value as \\[ P(Z \\geq z | H_0) = 1 - \\Phi(z)\\,, \\] where \\(z\\) is the realization of \\(Z\\) given that the realization of \\(X\\) is \\(x = 20\\). In R: n &lt;- 380 z &lt;- (200 - n * p0)/sqrt(n * p0 * (1-p0)) pnorm(z, lower.tail = FALSE) ## [1] 0.06016385 or, using the R proportion test prop.test ?prop.test prop.test(200, n, p = p0) # uses continuity correction ## ## 1-sample proportions test with continuity correction ## ## data: 200 out of n, null probability p0 ## X-squared = 2.2562, df = 1, p-value = 0.1331 ## alternative hypothesis: true p is not equal to 0.4864865 ## 95 percent confidence interval: ## 0.4747919 0.5772992 ## sample estimates: ## p ## 0.5263158 Notice that the result is not exactly the same. This is due to the fact that we are approximating a discrete probability sum with an integral of a Gaussian. To correct for this, it’s enough to subtract (or add, depending on the case) .5 to the \\(x\\) value. z_corr &lt;- (200 - .5 - n * p0)/sqrt(n * p0 * (1-p0)) pnorm(z_corr, lower.tail = FALSE) ## [1] 0.06653798 The bilateral case with Normal approximation is left to the reader as exercise "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
